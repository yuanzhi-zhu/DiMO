output_dir: "/Meissonic_discrete"
wandb_project: "DiMO_meissonic"
wandb_user: ""

is_debug: False

global_seed: 42

data_path: ../aesthetics
# model paths
pretrained_model_name_or_path: "meissonflow/meissonic"
text_encoder_architecture: 'open_clip'
pretrained_model_architecture: 'Meissonic'

# data
train_batch_size: 4
resolution: 1024
# data_path: ''     # path to prompts when data-free training
num_workers: 8

# training
resume_from_checkpoint_path: ""
mixed_precision: 'bf16'   # ["no", "fp16", "bf16"]
cast_models_to_dtype: false
gradient_accumulation_steps: 1
max_grad_norm: 1.
lr_scheduler_type: 'constant_with_warmup'
lr_warmup_steps: 100
max_train_steps:      20000
gradient_checkpointing: true
enable_xformers_memory_efficient_attention: false
# reuse_batch: false
ema_decay: 0.9999
ema_cpu: false
ema_freq: 1
loss_reduction: 'sum'     # ['sum', 'mean']
no_progress_bar: true

# optimizer
optimizer_type: 'adam'
adam_beta1:     0.9
adam_beta2:     0.999
adam_weight_decay: 0.

# log and save
validation_steps:       200
validation_steps_tuple: [2, 50, 200, 300, 500]
checkpointing_steps:  1000
report_to: "tensorboard"
checkpoints_total_limit: 5
checkpointing_steps_tuple: [3000, 5000, 8000, 10000, 15000, 20000, 25000, 50000, 80000, 100000]

# metric
metric_type: ''   # ["fid", "clip", "fid_clip", ""]
metric_steps: 5000
metric_steps_tuple: [2000, 5000]
metric_prompts: './prompts/captions.txt'

# generator
generator_lr:    1e-6
dm_loss_weight: 1.
fake_rounds: 1

# DiMO
fixed_ratio: 0.5       # fix ratio to be masked initial input
noise_emb_perturb: fix_0.3    # perturb init embedding with Gaussian noise E.g ['fix_0.', 'fix_0.3', 'warmup_15000']
distil_loss_type: 'Jeffreys_0.'  # ['FKL', 'RKL', 'Jeffreys']
ratio_mode: 'cosine'    # ['linear', 'square', 'cosine', 'arccos', 'mage']
ratio_mode_fake: 'cosine'
top_k: 0
top_p: 0.
fake_lr:    1e-6
true_cfg: 4.
fake_cfg_eval: 1.
fake_cfg_train: 1.
fake_cfg_drop_ratio: 0.1
gen_temp: 1.
true_temp: 1.
fake_temp: 1.
temperature_fake: 1.
alpha_fake: 0.          # 0 for hard loss, 1 for soft loss
fix_emb_layer: True
adaptive_cfg: False
distil_neg_prompt: True   # teacher has much better performance with negative prompt

# sampling
sched_mode: arccos
sampling_step: 64
cfg_w: 3
r_temp: 4.5
sm_temp: 1.

ratings: "all"